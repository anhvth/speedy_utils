{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147c23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6364a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from llm_utils import *\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "from llm_utils.chat_format.display import get_conversation_one_turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998e3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = AsyncLM(port=8140)\n",
    "# slm = LM(port=8140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add499f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseModel(BaseModel):\n",
    "    response: str = Field(..., description=\"The response from the LLM\")\n",
    "response = await lm.parse(ResponseModel, prompt=\"Write a short story about a cat\",\n",
    "                          instruction=\"You are a helpful assitant\", think=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02632329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_chat_messages_as_html(response['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab42db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-08 07:19:27.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mllm_utils.lm.async_lm\u001b[0m:\u001b[36mlist_models\u001b[0m:\u001b[36m714\u001b[0m - \u001b[34m\u001b[1mBase URL: http://localhost:8140/v1/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-32B-FP8'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(await lm.list_models(8140))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4f6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client: Start streaming chat completions...\n",
      "\n",
      "content:reasoning_content:\n",
      "\n",
      "\n",
      "\n",
      "reasoning_content:\n",
      "\n",
      "\n",
      "\n",
      "To compare **9.11** and **9.8**, follow these steps:\n",
      "\n",
      "1. **Look at the whole number part**:\n",
      "   - Both numbers have the same whole number part: **9**.\n",
      "\n",
      "2. **Compare the decimal parts**:\n",
      "   - **9.11** has a decimal part of **.11**.\n",
      "   - **9.8** has a decimal part of **.8**.\n",
      "\n",
      "3. **Convert decimals to the same place value for easier comparison**:\n",
      "   - **.8** is the same as **.80**.\n",
      "   - Now compare **.11** and **.80**.\n",
      "   - **.80 is greater than .11**.\n",
      "\n",
      "**Conclusion**: **9.8 is greater than 9.11**."
     ]
    }
   ],
   "source": [
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\"\"\"\n",
    "An example shows how to generate chat completions from reasoning models\n",
    "like DeepSeekR1.\n",
    "\n",
    "To run this example, you need to start the vLLM server with the reasoning \n",
    "parser:\n",
    "\n",
    "```bash\n",
    "vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "     --enable-reasoning --reasoning-parser deepseek_r1\n",
    "```\n",
    "\n",
    "Unlike openai_chat_completion_with_reasoning.py, this example demonstrates the\n",
    "streaming chat completions feature.\n",
    "\n",
    "The streaming chat completions feature allows you to receive chat completions\n",
    "in real-time as they are generated by the model. This is useful for scenarios\n",
    "where you want to display chat completions to the user as they are generated\n",
    "by the model.\n",
    "\n",
    "Remember to check content and reasoning_content exist in `ChatCompletionChunk`,\n",
    "content may not exist leading to errors if you try to access it.\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8140/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater? /no_think\"}]\n",
    "stream = client.chat.completions.create(model=model,\n",
    "                                        messages=messages,\n",
    "                                        stream=True)\n",
    "\n",
    "print(\"client: Start streaming chat completions...\")\n",
    "printed_reasoning_content = False\n",
    "printed_content = False\n",
    "\n",
    "for chunk in stream:\n",
    "    reasoning_content = None\n",
    "    content = None\n",
    "    # Check the content is reasoning_content or content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        reasoning_content = chunk.choices[0].delta.reasoning_content\n",
    "    elif hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        content = chunk.choices[0].delta.content\n",
    "\n",
    "    if reasoning_content is not None:\n",
    "        if not printed_reasoning_content:\n",
    "            printed_reasoning_content = True\n",
    "            print(\"reasoning_content:\", end=\"\", flush=True)\n",
    "        print(reasoning_content, end=\"\", flush=True)\n",
    "    elif content is not None:\n",
    "        if not printed_content:\n",
    "            printed_content = True\n",
    "            print(\"\\ncontent:\", end=\"\", flush=True)\n",
    "        # Extract and print the content\n",
    "        print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4383bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_current_temperature(location: str, unit: str = \"celsius\"):\n",
    "    \"\"\"Get current temperature at a location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, State, Country\".\n",
    "        unit: The unit to return the temperature in. Defaults to \"celsius\". (choices: [\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "    Returns:\n",
    "        the temperature, the location, and the unit in a dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"temperature\": 26.1,\n",
    "        \"location\": location,\n",
    "        \"unit\": unit,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_temperature_date(location: str, date: str, unit: str = \"celsius\"):\n",
    "    \"\"\"Get temperature at a location and date.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, State, Country\".\n",
    "        date: The date to get the temperature for, in the format \"Year-Month-Day\".\n",
    "        unit: The unit to return the temperature in. Defaults to \"celsius\". (choices: [\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "    Returns:\n",
    "        the temperature, the location, the date and the unit in a dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"temperature\": 25.9,\n",
    "        \"location\": location,\n",
    "        \"date\": date,\n",
    "        \"unit\": unit,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_function_by_name(name):\n",
    "    if name == \"get_current_temperature\":\n",
    "        return get_current_temperature\n",
    "    if name == \"get_temperature_date\":\n",
    "        return get_temperature_date\n",
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_temperature\",\n",
    "            \"description\": \"Get current temperature at a location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": 'The location to get the temperature for, in the format \"City, State, Country\".',\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": 'The unit to return the temperature in. Defaults to \"celsius\".',\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature_date\",\n",
    "            \"description\": \"Get temperature at a location and date.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": 'The location to get the temperature for, in the format \"City, State, Country\".',\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": 'The date to get the temperature for, in the format \"Year-Month-Day\".',\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": 'The unit to return the temperature in. Defaults to \"celsius\".',\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"date\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "MESSAGES = [\n",
    "    {\"role\": \"user\",  \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d9b44e",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'object': 'error', 'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mQwen/Qwen3-32B-FP8\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m client = lm.client\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m      6\u001b[39m     model=model_name,\n\u001b[32m      7\u001b[39m     messages=messages,\n\u001b[32m      8\u001b[39m     tools=tools,\n\u001b[32m      9\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m     10\u001b[39m     top_p=\u001b[32m0.8\u001b[39m,\n\u001b[32m     11\u001b[39m     max_tokens=\u001b[32m512\u001b[39m,\n\u001b[32m     12\u001b[39m     extra_body={\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrepetition_penalty\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1.05\u001b[39m,\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchat_template_kwargs\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33menable_thinking\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# default to True\u001b[39;00m\n\u001b[32m     15\u001b[39m     },\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/POLY/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/POLY/.venv/lib/python3.12/site-packages/openai/_base_client.py:1784\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1772\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1779\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1780\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1781\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1782\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1783\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/POLY/.venv/lib/python3.12/site-packages/openai/_base_client.py:1584\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1581\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1583\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1584\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1586\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'object': 'error', 'message': '\"auto\" tool choice requires --enable-auto-tool-choice and --tool-call-parser to be set', 'type': 'BadRequestError', 'param': None, 'code': 400}"
     ]
    }
   ],
   "source": [
    "messages = MESSAGES\n",
    "tools = TOOLS\n",
    "model_name = 'Qwen/Qwen3-32B-FP8'\n",
    "client = lm.client\n",
    "response = await client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    max_tokens=512,\n",
    "    extra_body={\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"chat_template_kwargs\": {\"enable_thinking\": False}  # default to True\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ef098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LM(port=8140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e2435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|im_start|>system\n",
    "# Tools\n",
    "\n",
    "You may call one or more functions to assist with the user query.\n",
    "\n",
    "You are provided with function signatures within <tools></tools> XML tags:\n",
    "<tools>\n",
    "{\"name\": \"get_current_temperature\", \"description\": \"Get current temperature at a location.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"}, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"}}, \"required\": [\"location\"]}}\n",
    "{\"name\": \"get_temperature_date\", \"description\": \"Get temperature at a location and date.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The location to get the temperature for, in the format \\\"City, State, Country\\\".\"}, \"date\": {\"type\": \"string\", \"description\": \"The date to get the temperature for, in the format \\\"Year-Month-Day\\\".\"}, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The unit to return the temperature in. Defaults to \\\"celsius\\\".\"}}, \"required\": [\"location\", \"date\"]}}\n",
    "</tools>\n",
    "\n",
    "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
    "</tool_call><|im_end|>\n",
    "<|im_start|>user\n",
    "What's the temperature in San Francisco now? How about tomorrow? Current Date: 2024-09-30.\n",
    "/no_think<|im_end|>\n",
    "<|im_start|>assistant\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07556237",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = lm.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db2e0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [\"/tool_call\", \"<|im_end|>\"]\n",
    "output = client.completions.create(model=\"Qwen/Qwen3-32B-FP8\",prompt=prompt, max_tokens=512,stop=stop_tokens)\n",
    "text = output.choices[0].text\n",
    "stop_reason = output.choices[0].stop_reason # type: ignore\n",
    "if stop_reason in stop_tokens:\n",
    "    text += stop_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c9cf707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\n<tool_call>\\n{\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"San Francisco, CA, US\", \"unit\": \"celsius\"}}\\n</tool_call'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e940875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
