{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147c23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1c01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6364a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils import LM\n",
    "from llm_utils import AsyncLM\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "from llm_utils.chat_format.display import get_conversation_one_turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998e3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = AsyncLM(port=8180)\n",
    "slm = LM(port=8180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94bc4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Output(BaseModel):\n",
    "    reasoning: str = Field(\n",
    "        description='The reasoning behind the answer',\n",
    "        examples=['To find the square root of 16, we look for a number that, '\n",
    "                  'when multiplied by itself, equals 16. The number 4 satisfies '\n",
    "                  'this condition.'],\n",
    "    )\n",
    "    result: float = Field(\n",
    "        description='The result of the calculation',\n",
    "        examples=[4.0],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3d47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = slm.parse(response_model=Output,\n",
    "                instruction=\"You are a calculator.\", prompt=\"What is the square root of pi?\",\n",
    "                think=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = await lm.parse(response_model=Output,\n",
    "                instruction=\"You are a calculator.\", prompt=\"What is the square root of i^2?\",\n",
    "                think=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92e88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = get_conversation_one_turn(system_msg=\"You are a calculator.\",\n",
    "                                      user_msg=\"What is the square root of i^2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a98a48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ab063321894763b6ba80ca4515fa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f588167611ff4e5581a96c5633e9e2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0c9e5a024542088849d4e21921fb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7743039b4fc94aecb5ca6770ee73118d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenzier = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-32B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd9b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;254;0;0m<|im_start|>system\u001b[0m \n",
      "\u001b[38;2;255;0;0mYou\u001b[0m \u001b[38;2;10;244;0mare\u001b[0m \u001b[38;2;222;32;0ma\u001b[0m \u001b[38;2;254;0;0mcalculator.<|im_end|>\u001b[0m \n",
      "\u001b[38;2;254;0;0m<|im_start|>user\u001b[0m \n",
      "\u001b[38;2;253;1;0mWhat\u001b[0m \u001b[38;2;21;233;0mis\u001b[0m \u001b[38;2;178;76;0mthe\u001b[0m \u001b[38;2;246;8;0msquare\u001b[0m \u001b[38;2;50;204;0mroot\u001b[0m \u001b[38;2;0;255;0mof\u001b[0m \u001b[38;2;243;11;0mi^2?<|im_end|>\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'word': '<|im_start|>system', 'probability': 0.002442209670151334},\n",
       "  {'word': '\\n', 'probability': 0.9666521128135672},\n",
       "  {'word': 'You', 'probability': 2.7253702561602877e-06},\n",
       "  {'word': 'are', 'probability': 0.9579636543157409},\n",
       "  {'word': 'a', 'probability': 0.1289727835311935},\n",
       "  {'word': 'calculator.<|im_end|>', 'probability': 0.00029386725891157736},\n",
       "  {'word': '\\n', 'probability': 0.0021631909319071137},\n",
       "  {'word': '<|im_start|>user', 'probability': 0.0004825715933300054},\n",
       "  {'word': '\\n', 'probability': 0.5902933453202143},\n",
       "  {'word': 'What', 'probability': 0.007008531551294818},\n",
       "  {'word': 'is', 'probability': 0.9149899764643808},\n",
       "  {'word': 'the', 'probability': 0.29890241069774487},\n",
       "  {'word': 'square', 'probability': 0.03519748045307566},\n",
       "  {'word': 'root', 'probability': 0.8020781429933217},\n",
       "  {'word': 'of', 'probability': 0.9979610377996376},\n",
       "  {'word': 'i^2?<|im_end|>', 'probability': 0.04416492057446249},\n",
       "  {'word': '\\n', 'probability': 0.9578824878388407}],\n",
       " [{'151644': {'logprob': -1, 'rank': 1, 'decoded_token': '<|im_start|>'}},\n",
       "  {'8948': {'logprob': -11.029704093933105,\n",
       "    'rank': 10435,\n",
       "    'decoded_token': 'system'}},\n",
       "  {'198': {'logprob': -0.03391660749912262, 'rank': 1, 'decoded_token': 'Ċ'}},\n",
       "  {'2610': {'logprob': -12.812906265258789,\n",
       "    'rank': 25,\n",
       "    'decoded_token': 'You'}},\n",
       "  {'525': {'logprob': -0.042945440858602524,\n",
       "    'rank': 1,\n",
       "    'decoded_token': 'Ġare'}},\n",
       "  {'264': {'logprob': -2.048153877258301, 'rank': 2, 'decoded_token': 'Ġa'}},\n",
       "  {'29952': {'logprob': -11.990222930908203,\n",
       "    'rank': 2421,\n",
       "    'decoded_token': 'Ġcalculator'}},\n",
       "  {'13': {'logprob': -1.885472297668457, 'rank': 2, 'decoded_token': '.'}},\n",
       "  {'151645': {'logprob': -10.521451950073242,\n",
       "    'rank': 356,\n",
       "    'decoded_token': '<|im_end|>'}},\n",
       "  {'198': {'logprob': -6.136170864105225, 'rank': 3, 'decoded_token': 'Ċ'}},\n",
       "  {'151644': {'logprob': -3.266281055402942e-05,\n",
       "    'rank': 1,\n",
       "    'decoded_token': '<|im_start|>'}},\n",
       "  {'872': {'logprob': -15.272729873657227,\n",
       "    'rank': 548,\n",
       "    'decoded_token': 'user'}},\n",
       "  {'198': {'logprob': -0.5271356701850891, 'rank': 1, 'decoded_token': 'Ċ'}},\n",
       "  {'3838': {'logprob': -4.96062707901001, 'rank': 6, 'decoded_token': 'What'}},\n",
       "  {'374': {'logprob': -0.08884216845035553,\n",
       "    'rank': 1,\n",
       "    'decoded_token': 'Ġis'}},\n",
       "  {'279': {'logprob': -1.207638144493103, 'rank': 2, 'decoded_token': 'Ġthe'}},\n",
       "  {'9334': {'logprob': -3.346780776977539,\n",
       "    'rank': 5,\n",
       "    'decoded_token': 'Ġsquare'}},\n",
       "  {'3704': {'logprob': -0.22054924070835114,\n",
       "    'rank': 1,\n",
       "    'decoded_token': 'Ġroot'}},\n",
       "  {'315': {'logprob': -0.0020410437136888504,\n",
       "    'rank': 1,\n",
       "    'decoded_token': 'Ġof'}},\n",
       "  {'600': {'logprob': -7.024777889251709, 'rank': 18, 'decoded_token': 'Ġi'}},\n",
       "  {'61': {'logprob': -4.579761981964111, 'rank': 9, 'decoded_token': '^'}},\n",
       "  {'17': {'logprob': -0.9873208999633789, 'rank': 1, 'decoded_token': '2'}},\n",
       "  {'30': {'logprob': -1.241650938987732, 'rank': 2, 'decoded_token': '?'}},\n",
       "  {'151645': {'logprob': -1.7656105756759644,\n",
       "    'rank': 2,\n",
       "    'decoded_token': '<|im_end|>'}},\n",
       "  {'198': {'logprob': -0.04303017258644104, 'rank': 1, 'decoded_token': 'Ċ'}}],\n",
       " '\\x1b[38;2;254;0;0m<|im_start|>system\\x1b[0m \\n\\x1b[38;2;255;0;0mYou\\x1b[0m \\x1b[38;2;10;244;0mare\\x1b[0m \\x1b[38;2;222;32;0ma\\x1b[0m \\x1b[38;2;254;0;0mcalculator.<|im_end|>\\x1b[0m \\n\\x1b[38;2;254;0;0m<|im_start|>user\\x1b[0m \\n\\x1b[38;2;253;1;0mWhat\\x1b[0m \\x1b[38;2;21;233;0mis\\x1b[0m \\x1b[38;2;178;76;0mthe\\x1b[0m \\x1b[38;2;246;8;0msquare\\x1b[0m \\x1b[38;2;50;204;0mroot\\x1b[0m \\x1b[38;2;0;255;0mof\\x1b[0m \\x1b[38;2;243;11;0mi^2?<|im_end|>\\x1b[0m')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slm.inspect_word_probs(messages=messages, tokenizer=tokenzier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de02169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;254;0;0m<|im_start|>system\u001b[0m \n",
      "\u001b[38;2;255;0;0mYou\u001b[0m \u001b[38;2;10;244;0mare\u001b[0m \u001b[38;2;222;32;0ma\u001b[0m \u001b[38;2;254;0;0mcalculator.<|im_end|>\u001b[0m \n",
      "\u001b[38;2;254;0;0m<|im_start|>user\u001b[0m \n",
      "\u001b[38;2;253;1;0mWhat\u001b[0m \u001b[38;2;21;233;0mis\u001b[0m \u001b[38;2;178;76;0mthe\u001b[0m \u001b[38;2;246;8;0msquare\u001b[0m \u001b[38;2;50;204;0mroot\u001b[0m \u001b[38;2;0;255;0mof\u001b[0m \u001b[38;2;243;11;0mi^2?<|im_end|>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await lm.inspect_word_probs(messages=messages, tokenizer=tokenzier);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e818c743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a calculator.\\n\\n/no_think'},\n",
       " {'role': 'user', 'content': 'What is the square root of pi?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '<think>\\n\\n</think>\\n{\"reasoning\":\"To find the square root of pi, I will calculate the square root of the mathematical constant pi (π), which is approximately 3.141592653589793.\",\"result\":1.7724538509055159}'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slm.last_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4426d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "await lm.last_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3e40020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default font size: 1px\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Model must be set before streaming.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDefault font size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlm.font_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mpx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Test streaming with improved HTML mode in Jupyter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHello /think\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhtml_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# show_input=True\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/POLY/src/.venv/lib/python3.12/site-packages/llm_utils/lm/chat_html.py:204\u001b[39m, in \u001b[36mLMChatHtml.chat_stream\u001b[39m\u001b[34m(self, prompt, messages, html_mode, font_size, padding, inner_padding, **kwargs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for type-checker\u001b[39;00m\n\u001b[32m    199\u001b[39m openai_msgs: Messages = (\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_messages(cast(LegacyMsgs, messages))\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages[\u001b[32m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m)  \u001b[38;5;66;03m# legacy style\u001b[39;00m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m cast(Messages, messages)  \u001b[38;5;66;03m# already typed\u001b[39;00m\n\u001b[32m    203\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mModel must be set before streaming.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m stream = \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m    207\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    208\u001b[39m     messages=openai_msgs,\n\u001b[32m    209\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    210\u001b[39m     **kwargs,\n\u001b[32m    211\u001b[39m )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    213\u001b[39m output_text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Model must be set before streaming."
     ]
    }
   ],
   "source": [
    "from llm_utils import get_conversation_one_turn\n",
    "from llm_utils.lm.chat_html import LMChatHtml as LM\n",
    "\n",
    "# Test with default font size\n",
    "lm = LM(port=8140)\n",
    "print(f\"Default font size: {lm.font_size}px\")\n",
    "# Test streaming with improved HTML mode in Jupyter\n",
    "response = lm.chat_stream(\n",
    "    prompt='Hello /think',\n",
    "    html_mode=True,\n",
    "    max_tokens=10000, \n",
    "    # show_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02632329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
