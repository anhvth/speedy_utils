{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136ff273",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the LLM-as-a-Judge system with structured prompts, variable substitution, and SFT export capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1bf59",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8f8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llm_utils import (\n",
    "    LLMJudgeBase, \n",
    "    Signature, \n",
    "    InputField, \n",
    "    OutputField\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaceb8bd",
   "metadata": {},
   "source": [
    "## Example 1: DSPy-like Signature System\n",
    "\n",
    "First, let's create a simple factual accuracy judge using the Signature system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b5e2123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Instruction:\n",
      "Judge if the answer is factually correct based on the context.\n",
      "\n",
      "**Input Fields:**\n",
      "- context (str): Context for the prediction\n",
      "- question (str): Question to be answered\n",
      "- answer (str): Answer for the question\n",
      "\n",
      "**Output Fields:**\n",
      "- factually_correct (bool): Is the answer factually correct based on the context?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Input Schema:\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"context\": {\n",
      "      \"description\": \"Context for the prediction\",\n",
      "      \"title\": \"Context\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"question\": {\n",
      "      \"description\": \"Question to be answered\",\n",
      "      \"title\": \"Question\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"answer\": {\n",
      "      \"description\": \"Answer for the question\",\n",
      "      \"title\": \"Answer\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"context\",\n",
      "    \"question\",\n",
      "    \"answer\"\n",
      "  ],\n",
      "  \"title\": \"FactJudgeInput\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "Output Schema:\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"factually_correct\": {\n",
      "      \"description\": \"Is the answer factually correct based on the context?\",\n",
      "      \"title\": \"Factually Correct\",\n",
      "      \"type\": \"boolean\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"factually_correct\"\n",
      "  ],\n",
      "  \"title\": \"FactJudgeOutput\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define a signature like DSPy (original syntax - no more type warnings!)\n",
    "class FactJudge(Signature):\n",
    "    \"\"\"Judge if the answer is factually correct based on the context.\"\"\"\n",
    "    \n",
    "    # No more type warnings with the updated InputField/OutputField!\n",
    "    context: str = InputField(desc=\"Context for the prediction\")\n",
    "    question: str = InputField(desc=\"Question to be answered\")\n",
    "    answer: str = InputField(desc=\"Answer for the question\")\n",
    "    factually_correct: bool = OutputField(desc=\"Is the answer factually correct based on the context?\")\n",
    "\n",
    "# Show the generated instruction\n",
    "print(\"Generated Instruction:\")\n",
    "print(FactJudge.get_instruction())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Show the input/output models (now always Pydantic models)\n",
    "input_model = FactJudge.get_input_model()\n",
    "output_model = FactJudge.get_output_model()\n",
    "\n",
    "print(\"Input Schema:\")\n",
    "print(json.dumps(input_model.model_json_schema(), indent=2))\n",
    "\n",
    "print(\"\\nOutput Schema:\")\n",
    "print(json.dumps(output_model.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "380542eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sig(Signature):\n",
    "    \"\"\"You are a careful **translation evaluator**.\n",
    "\n",
    "You are given five inputs:\n",
    "\n",
    "* **Source Prompt** (the original text & any constraints)\n",
    "* **AI Translation** (the machine translation to evaluate)\n",
    "* **Human Reference** (a reference rendering; use only for guidance, not as ground truth)\n",
    "* **System Message** (an automated hint about a possible structural error)\n",
    "* **Glossaries** (optional terminology constraints; may be empty)\n",
    "\n",
    "## Your tasks\n",
    "\n",
    "1. **Check structure correctness**:\n",
    "   - Use the System Message as a hint.\n",
    "   - Assign a `structure_score`:\n",
    "     * `0` = structure is clearly wrong or the error flagged is correct.\n",
    "     * `1` = partially correct but flawed.\n",
    "     * `2` = structure is correct; the system error is invalid.\n",
    "\n",
    "2. **Check translation quality**:\n",
    "   - Compare AI Translation with Source Prompt and Human Reference.\n",
    "   - Assign a `translation_score`:\n",
    "     * `0` = unfaithful (major omissions/additions/distortions/repetitions).\n",
    "     * `1` = somewhat faithful (mostly correct but noticeable issues).\n",
    "     * `2` = faithful (preserves meaning, scope, nuance; only minor style differences).\n",
    "\n",
    "3. **Check glossary/terminology adherence**:\n",
    "   - If no glossary is provided → `term_score = 2`.\n",
    "   - If glossary exists but only partially followed → `term_score = 1`.\n",
    "   - If glossary exists but not followed at all → `term_score = 0`.\n",
    "\n",
    "## Output format (JSON only; no commentary)\n",
    "\n",
    "{{\"structure_score\": <0|1|2>, \"translation_score\": <0|1|2>, \"term_score\": <0|1|2>}}\n",
    "\n",
    "* Return exactly one JSON object.\n",
    "* Do not output any explanations.\n",
    "\"\"\"\n",
    "    SOURCE_PROMPT: str = InputField(desc=\"The original text to be translated, along with any constraints.\")\n",
    "    AI_TRANSLATION: str = InputField(desc=\"The machine translation output to be evaluated.\")\n",
    "    HUMAN_REFERENCE: str = InputField(desc=\"A reference human translation, to be used for guidance but not as ground truth.\")\n",
    "    SYSTEM_MESSAGE: str = InputField(desc=\"An automated hint about a possible structural error in the AI translation.\")\n",
    "    GLOSSARIES: str = InputField(desc=\"Optional terminology constraints; may be empty.\")\n",
    "    \n",
    "    structure_score: int = OutputField(desc=\"Score for structural correctness: 0 (wrong), 1 (partially correct), 2 (correct)\")\n",
    "    glossary_score: int = OutputField(desc=\"Score for glossary adherence: 0 (not followed), 1 (partially followed), 2 (fully followed or no glossary)\")\n",
    "    translation_score: int = OutputField(desc=\"Score for translation quality: 0 (unfaithful), 1 (somewhat faithful), 2 (faithful)\")\n",
    "        \n",
    "# --- Updated evaluation prompt ---\n",
    "\n",
    "import os\n",
    "judge = LLMJudgeBase(signature=Sig, client=8000) # vllm is hosted at port 8000\n",
    "judge = LLMJudgeBase(signature=Sig, model='gpt-4.1-mini', client=None) # use openai's gpt-4.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "288ebc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Sig.get_input_model()(\n",
    "    SOURCE_PROMPT=\"Translate the following English text to French, ensuring that the structure is preserved and the terminology is accurate. The text is: 'The quick brown fox jumps over the lazy dog.'\",\n",
    "    AI_TRANSLATION=\"Le renard brun rapide saute par-dessus le chien paresseux.\",\n",
    "    HUMAN_REFERENCE=\"Le vif renard brun bondit par-dessus le chien paresseux.\",\n",
    "    SYSTEM_MESSAGE=\"The AI translation has a structural error: it uses 'rapide' instead of 'vif' to describe the fox, which affects the nuance of the sentence.\",\n",
    "    GLOSSARIES=\"vif: quick, lively; paresseux: lazy\",\n",
    ")\n",
    "output = judge(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70d221c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLMJudgeBase' object has no attribute 'inspect_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjudge\u001b[49m\u001b[43m.\u001b[49m\u001b[43minspect_history\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'LLMJudgeBase' object has no attribute 'inspect_history'"
     ]
    }
   ],
   "source": [
    "judge.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2c995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Okay, let's start by looking at the structure. The system message says the AI translation used 'rapide' instead of 'vif', which affects the nuance. The original prompt specified using accurate terminology. The glossary provides 'vif' for 'quick', so the AI should have used 'vif' instead of 'rapide'. That's a structural issue because the term choice is specified. So structure_score is 0.\n",
      "\n",
      "Next, translation quality. The AI's translation is mostly correct but uses the wrong term. The human reference uses 'vif', which is the correct term according to the glossary. The meaning is preserved, but the nuance is off. So translation_score is 1 because it's somewhat faithful but has a noticeable issue.\n",
      "\n",
      "Glossary adherence: The glossary specifies 'vif' for 'quick', but the AI used 'rapide'. So the term wasn't followed. Term_score is 0.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16f277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speedy-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
