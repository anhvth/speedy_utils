# Zero-Copy Sharing with Ray

The `multi_process` function now supports zero-copy sharing of large objects via Ray's object store when using the Ray backend.

## Overview

When processing data in parallel, you often need to share large objects (models, datasets, configuration) across all workers. Without optimization, these objects are serialized and copied to each worker, wasting memory and time.

With `shared_kwargs`, large objects are placed in Ray's distributed object store once and shared across all workers via zero-copy (for numpy arrays and other supported types).

## Basic Usage

```python
from speedy_utils import multi_process
import numpy as np

# Create a large object to share
large_model = {
    'weights': np.random.randn(1000, 1000),  # 8 MB
    'bias': np.array([1.5])
}

def process_item(item_id, model=None):
    # Use the shared model
    return model['weights'][item_id % 100, 0] + model['bias'][0]

# Use shared_kwargs to enable zero-copy sharing
results = multi_process(
    process_item,
    items=range(100),
    workers=4,
    backend='ray',
    shared_kwargs=['model'],  # Enable zero-copy for 'model'
    model=large_model
)
```

## Performance Benefits

From the example above with an 8 MB model:
- **Without sharing**: 3.196s (model copied to each worker)
- **With zero-copy**: 0.205s (model shared via object store)
- **Speedup**: 15.58x faster! ðŸš€

The speedup increases with:
1. Larger object sizes
2. More workers
3. More tasks processed

## Validation

The function validates `shared_kwargs` to prevent errors:

```python
# âŒ Error: 'large_array' not in func_kwargs
multi_process(
    my_func, items, 
    shared_kwargs=['large_array']  # Missing from kwargs!
)

# âŒ Error: 'invalid' is not a valid parameter
multi_process(
    my_func, items,
    shared_kwargs=['invalid'],  # Not a parameter of my_func
    invalid=data
)

# âŒ Error: shared_kwargs only works with Ray backend
multi_process(
    my_func, items,
    backend='mp',  # Must use 'ray' backend
    shared_kwargs=['data'],
    data=large_data
)
```

## Multiple Shared Objects

Share multiple large objects:

```python
model = {'weights': np.random.randn(500, 500)}
lookup_table = np.random.randn(1000, 100)
config = {'threshold': 0.5}

results = multi_process(
    process_func,
    items,
    workers=4,
    backend='ray',
    shared_kwargs=['model', 'lookup'],  # Share both
    model=model,
    lookup=lookup_table,
    config=config  # Small - no need to share
)
```

## Memory Efficiency

**Without sharing** (32 MB object, 4 workers):
- Each worker: 32 MB copy
- Total: 128 MB (4 Ã— 32 MB)

**With zero-copy**:
- Object store: 32 MB (shared)
- Total: 32 MB
- **Memory saved**: 96 MB (75% reduction)

## Zero-Copy Details

Ray's zero-copy works best for:
- âœ… **NumPy arrays** - True zero-copy (backed by shared memory)
- âœ… Large dictionaries with numpy arrays
- âœ… Custom objects containing numpy arrays
- âš ï¸ Other types - Still faster than copying, but may deserialize

## Best Practices

1. **Use for large objects only** (> 1 MB)
   - Small objects have negligible overhead
   
2. **Share read-only data**
   - Shared objects are immutable in Ray
   
3. **Prefer numpy arrays**
   - True zero-copy for numpy arrays
   
4. **Validate parameters**
   - The function auto-validates `shared_kwargs`
   
5. **Monitor memory**
   - Ray's object store has a size limit
   
## Common Use Cases

### 1. Machine Learning Models
```python
import torch

model = torch.load('model.pth')
model.eval()

results = multi_process(
    inference_func,
    items=images,
    shared_kwargs=['model'],
    model=model
)
```

### 2. Large Datasets
```python
# Share a large reference dataset
reference_data = np.load('reference.npy')  # 100 MB

results = multi_process(
    compare_func,
    items=samples,
    shared_kwargs=['reference'],
    reference=reference_data
)
```

### 3. Lookup Tables
```python
# Share embedding lookup table
embeddings = np.random.randn(10000, 512)  # Large embeddings

results = multi_process(
    embed_func,
    items=token_ids,
    shared_kwargs=['embeddings'],
    embeddings=embeddings
)
```

## API Reference

### Parameters

- **shared_kwargs**: `list[str] | None`
  - List of kwarg names to share via Ray's object store
  - Must be valid parameters of the function
  - Must be provided in `func_kwargs`
  - Only works with `backend='ray'`
  - Default: `None` (no sharing)

### Errors

- `ValueError`: If shared_kwargs key not in func_kwargs
- `ValueError`: If shared_kwargs key not a valid function parameter
- `ValueError`: If shared_kwargs used with non-Ray backend

## Example: End-to-End

```python
from speedy_utils import multi_process
import numpy as np

def classify_image(img_id, model=None, config=None):
    """Classify an image using a shared model."""
    # Model is shared - no copy overhead
    features = model['weights'][img_id % 100]
    threshold = config['threshold']
    
    return 1 if features.mean() > threshold else 0

# Setup
model = {
    'weights': np.random.randn(1000, 512),  # 4 MB
    'name': 'ResNet50'
}
config = {'threshold': 0.5, 'batch_size': 32}

# Process with zero-copy sharing
results = multi_process(
    classify_image,
    items=range(1000),
    workers=8,
    backend='ray',
    shared_kwargs=['model'],  # Share model only
    model=model,
    config=config,  # Small - copied is fine
    desc='Classifying images'
)

print(f'Classified {len(results)} images')
print(f'Positive: {sum(results)}')
```

## See Also

- [Ray Objects Documentation](https://docs.ray.io/en/latest/ray-core/objects.html)
- `examples/shared_kwargs_example.py` - Complete examples
- `tests/test_shared_kwargs.py` - Test suite
